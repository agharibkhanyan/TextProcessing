import re

def tokenize(path):
    tokenList = []
    tempLine = path.readlines()

    for x in tempLine:
        token = re.findall(r'\w+', x)
        tokenList +=token

    tokenList = [x.lower().strip() for x in tokenList]

    for i in tokenList:
        print(i)

    return tokenList

def  computeWordFrequencies(tokens):

    temp = []
    for i in tokens:
        if i not in temp:
            temp.append(i)

    for i in range(0,len(temp)):
        print(tokens.count(temp[i]))

file0 = open("C:/Users/anon/Documents/test.txt","r",encoding='ascii', errors='replace')
tokens = tokenize(file0)
#computeWordFrequencies(tokens)

#for i in tokens:
  #  print(i)

#file0.close()